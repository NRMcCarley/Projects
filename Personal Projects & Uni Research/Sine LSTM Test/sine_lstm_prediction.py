# -*- coding: utf-8 -*-
"""Sine LSTM Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K_Tp7ooL6hNj1B280bwXd4eHyIms--jR

# **0. Import Statements**
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Dense, Dropout
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.preprocessing import StandardScaler
import seaborn as sns
from datetime import datetime

"""# **1. Create Basic "Historic" Data**"""

xs = np.arange(0, 20, 0.1)
data = np.sin(xs)
data = data.reshape(len(data), 1)
num_features = np.shape((data))[1]
# print(data)
plt.plot(xs,data)

"""# **2. Scale Data For Training**"""

# LSTM uses sigmoid and tanh that are sensitive to magnitude, so values need to
# be normalized
scaler = StandardScaler()
scaler = scaler.fit(data)
data_scaled = scaler.transform(data)
plt.plot(xs, data_scaled)

# dim of trainX = (samples, timesteps, num days to forecast, num input vars)
# dim of trainY = (time steps - num days to forecast, num prediction outputs)

num_data_points = np.shape(data_scaled)[0]
lookback = 20  # days to look back at for the forecast
num_samples = num_data_points - lookback - 1

X = np.zeros((num_samples, lookback, num_features))
Y = np.zeros((num_samples, num_features))

i = 0
while i < num_samples:
  tempX = data_scaled[i:i+lookback,:]
  tempY = data_scaled[i+lookback,:]
  X[i,:,:] = tempX
  Y[i,:] = tempY
  i += 1

print('X shape == {}.'.format(X.shape))
print('Y shape == {}.'.format(Y.shape))

"""# **3. Separate Data Into trainX & trainY**"""

num_testing_samples = 140
trainX = X[0:num_testing_samples,:,:]
testX = X[num_testing_samples:,:,:]
trainY = Y[0:num_testing_samples,:]
testY = Y[num_testing_samples:,:]

"""# **4. Create Model (Sequential)**"""

# define the Autoencoder model

n_neurons = 32
model = Sequential()
model.add(LSTM(n_neurons, input_shape=(trainX.shape[1], trainX.shape[2])))
model.add(Dense(trainY.shape[1]))
model.compile(optimizer='adam', loss='mse')
model.summary()

# log_dir = "logs/fit/" + datetime.now().strftime("%Y%m%d-%H%M%S")
# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

"""# **5. Fit Model**"""

# fit the model
# history = model.fit(trainX, trainY, epochs=6, batch_size=16, validation_split=0.1, verbose=1, callbacks=[tensorboard_callback])
history = model.fit(trainX, trainY, epochs=300, verbose=0)

plt.plot(history.history['loss'], label='Training loss')
# plt.plot(history.history['val_loss'], label='Validation loss')
plt.legend()

"""# **6. Make Predictions**"""

#Make prediction
test_pred = model.predict(testX) #shape = (n, 1) where n is the n_days_for_prediction
test_pred_scaled = scaler.inverse_transform(test_pred)

num_predictions = np.shape(Y)[0]-np.shape(trainY)[0]
trueY = data[-num_predictions:,:]
plot_x = np.arange(num_predictions)
plt.plot(plot_x, trueY, label="Real")
plt.plot(plot_x, test_pred_scaled, label="Predicted")
plt.legend()
plt.title("Real vs Predicted")
plt.show()

plt.plot(plot_x, np.abs((trueY-test_pred_scaled)))
plt.title("Error at Each Step")
plt.show()

a = data[-num_predictions:,:]
print(a)