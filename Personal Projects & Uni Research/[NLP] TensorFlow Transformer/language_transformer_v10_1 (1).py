# -*- coding: utf-8 -*-
"""Language Transformer v10.1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lFUXNxvF40sjte8KDda1rvBFS7ygU0Wy
"""

pip install --upgrade tensorflow

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import Embedding, MultiHeadAttention, LayerNormalization, Dropout, Dense
from tensorflow.keras.models import Sequential
print(tf.__version__)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

D_MODEL = 512                      # Model dimension
NUM_HEADS = 8                      # Number of heads
D_K = int(D_MODEL / NUM_HEADS)     # Dimension of Q and K
D_V = D_K                          # Dimension of V
NUM_LAYERS = 3                     # Number of Encoder/Decoder Stacks
D_FF = 2048                        # Number of neurons in Feed Forward layer
MAX_TOKENS = 100                   # Maximum number of tokens in a sequence
DROPOUT_RATE = 0.1                 # Gloabl dropout rate
maxTokenLen = 25
epsilon = 0.0000001

"""# **1. Data**"""

sheet_url = "https://docs.google.com/spreadsheets/d/1jipqsnc_L5Il90fX93yYtTlr_H7tNFB_Z5N2U2WsafM/edit#gid=0"
url_1 = sheet_url.replace('/edit#gid=', '/export?format=csv&gid=')

df = pd.read_csv(url_1)
space = ' '
df = df.append({'Word':space}, ignore_index=True)
startToken = tf.convert_to_tensor([['<']])
words = list(df)
df_vocab = df[words].astype(str)
Vocab = df_vocab.to_numpy()

VOCAB_SIZE = np.shape(Vocab)[0]

def getVocabIdx(word):
  # Takes in str of word, return int of vocab index
  return df.index[df['Word'] == word].to_list()[0]

token_length_to_index = {}
for idx, row in df.iterrows():
    if pd.notna(row['Word']):
        token_len = len(row['Word'])
        if token_len not in token_length_to_index:
            token_length_to_index[token_len] = []
        token_length_to_index[token_len].append((row['Word'], getVocabIdx(row['Word'])))

def tokenize_and_pad_training(inputStrList):
  # Returns array of (samples, MAX_TOKENS+1) in order to get the right dimensions for Y
  global MAX_TOKENS
  # Initialize an empty list to store tokenized and padded sequences
  tokenized_sequences = []

  for inputStr in inputStrList:
      tokens = []
      charIdx = 0
      numChars = len(inputStr)

      while charIdx < numChars:
          found_token = False
          for windowSize in range(min(maxTokenLen, numChars - charIdx), 0, -1):
              token = inputStr[charIdx:charIdx + windowSize]
              if token_length_to_index.get(windowSize) is not None:
                  for word, vocab_idx in token_length_to_index[windowSize]:
                      if word == token:
                          tokens.append(vocab_idx)
                          charIdx += windowSize
                          found_token = True
                          break
              if found_token:
                  break
          if not found_token:
              charIdx += 1

      # Add start (1) and end (2) tokens before and after the sentence
      tokens = [1] + tokens + [2]

      # Pad or truncate the tokens to maxSeqLen
      tokens = tokens[:MAX_TOKENS] + [0] * (MAX_TOKENS+1 - len(tokens))

      tokenized_sequences.append(tokens)

  # Convert the list of tokenized sequences to a NumPy array
  output = np.array(tokenized_sequences)

  return output

textX = ['What is the meaning of life?',
        'How does photosynthesis work?',
        'What is gravity?',
        'How many eggs are in a dozen?',
        'What is the capital of France?']
textY = ['The meaning of life is a philosophical question that has been debated for centuries.',
        'Photosynthesis is the process by which plants use sunlight, carbon dioxide, and water to produce glucose (sugar) and oxygen.',
        'Gravity is the force of attraction between two objects with mass.',
        'There are twelve eggs in a dozen.',
        'The capital of France is Paris.']

tokensX = tokenize_and_pad_training(textX)[:, 0:MAX_TOKENS]
tokensY = tokenize_and_pad_training(textY)

inputX = tokensX
inputY = tokensY[:,:-1]
trueY = tokensY[:,1:]

inputX = inputX.astype(np.int32)
inputY = inputY.astype(np.int32)
inputY[inputY == 2] = 0
trueY = trueY.astype(np.int32)

# testing
sample = 1

inputX[sample,:]

inputY[sample,:]

trueY[sample,:]

NUM_SAMPLES = np.shape(inputX)[0]

numToTrain = int(np.round(0.8 * NUM_SAMPLES))

inputX_train = inputX[0:numToTrain, :]
inputX_val = inputX[numToTrain:NUM_SAMPLES+1, :]

inputY_train = inputY[0:numToTrain,:]
inputY_val = inputY[numToTrain:NUM_SAMPLES+1, :]

trueY_train = trueY[0:numToTrain,:]
trueY_val = trueY[numToTrain:NUM_SAMPLES+1, :]

BATCH_SIZE = 1

# Prepare the training dataset.
train_dataset = tf.data.Dataset.from_tensor_slices(((inputX_train, inputY_train), trueY_train))     # slices along the first axis of both sets
train_dataset = train_dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE)     # creates batches of slices, of size batch_size

# Prepare the validation dataset.
val_dataset = tf.data.Dataset.from_tensor_slices(((inputX_val, inputY_val), trueY_val))
val_dataset = val_dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE)

train_dataset
val_dataset

"""#**2. Testing the Functions**"""

for (batch, ((x_batch_train, y_batch_train), true_y_batch_train)) in enumerate(train_dataset):
  break

print(x_batch_train)
print(y_batch_train)
print(true_y_batch_train)

"""2.1 Embeddings"""

def positional_encoding(length, depth):
  depth = depth/2

  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)
  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)

  angle_rates = 1 / (10000**depths)         # (1, depth)
  angle_rads = positions * angle_rates      # (pos, depth)

  pos_encoding = np.concatenate(
      [np.sin(angle_rads), np.cos(angle_rads)],
      axis=-1)

  return tf.cast(pos_encoding, dtype=tf.float32)


class PositionalEmbedding(tf.keras.layers.Layer):
  def __init__(self, vocab_size, d_model):
    super().__init__()
    self.d_model = d_model
    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)  # Add mask_zero=True here
    self.pos_encoding = positional_encoding(length=2048, depth=d_model)

  def compute_mask(self, inputs, mask=None):
    # The mask returned by this layer is the same as the one received (identity mask).
    return mask

  def call(self, x, mask=None):
    length = tf.shape(x)[1]
    x = self.embedding(x)
    # This factor sets the relative scale of the embedding and positional_encoding.
    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
    x = x + self.pos_encoding[tf.newaxis, :length, :]
    return x

# We only use one embedder since the input and output vocab are the same
# (use two embedders if it's a vision transformer, one language to another
# language, etc)
gen_embed = PositionalEmbedding(vocab_size=VOCAB_SIZE, d_model=D_MODEL)

x_emb = gen_embed(x_batch_train)
y_emb = gen_embed(y_batch_train)

print(f'x_emb shape: {x_emb.shape}')
print(f'y_emb shape: {y_emb.shape}')

"""2.2 Attention Types"""

class BaseAttention(tf.keras.layers.Layer):
  def __init__(self, **kwargs):
    super().__init__()
    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)
    self.layernorm = tf.keras.layers.LayerNormalization()
    self.add = tf.keras.layers.Add()

class CrossAttention(BaseAttention):
  def call(self, x, context):
    attn_output, attn_scores = self.mha(
        query=x,
        key=context,
        value=context,
        return_attention_scores=True)

    # Cache the attention scores for plotting later.
    self.last_attn_scores = attn_scores

    x = self.add([x, attn_output])
    x = self.layernorm(x)

    return x

sample_ca = CrossAttention(num_heads=2, key_dim=512)

print(sample_ca(x_emb, y_emb).shape)

class GlobalSelfAttention(BaseAttention):
  def call(self, x):
    attn_output = self.mha(
        query=x,
        value=x,
        key=x)
    x = self.add([x, attn_output])
    x = self.layernorm(x)
    return x

sample_gsa = GlobalSelfAttention(num_heads=2, key_dim=512)

print(sample_gsa(x_emb).shape)

class CausalSelfAttention(BaseAttention):
  def call(self, x):
    attn_output = self.mha(
        query=x,
        value=x,
        key=x,
        use_causal_mask = True)
    x = self.add([x, attn_output])
    x = self.layernorm(x)
    return x

sample_csa = CausalSelfAttention(num_heads=2, key_dim=512)

print(sample_csa(y_emb).shape)

"""Feed Forward"""

class FeedForward(tf.keras.layers.Layer):
  def __init__(self, d_model, dff, dropout_rate=0.1):
    super().__init__()
    self.seq = tf.keras.Sequential([
      tf.keras.layers.Dense(dff, activation='relu'),
      tf.keras.layers.Dense(d_model),
      tf.keras.layers.Dropout(dropout_rate)
    ])
    self.add = tf.keras.layers.Add()
    self.layer_norm = tf.keras.layers.LayerNormalization()

  def call(self, x):
    x = self.add([x, self.seq(x)])
    x = self.layer_norm(x)
    return x

sample_ffn = FeedForward(512, 2048)

print(sample_ffn(y_emb).shape)

"""Encoder Layer"""

class EncoderLayer(tf.keras.layers.Layer):
  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):
    super().__init__()

    self.self_attention = GlobalSelfAttention(
        num_heads=num_heads,
        key_dim=d_model,
        dropout=dropout_rate)

    self.ffn = FeedForward(d_model, dff)

  def call(self, x):
    x = self.self_attention(x)
    x = self.ffn(x)
    return x

sample_encoder_layer = EncoderLayer(d_model=512, num_heads=8, dff=2048)

print(sample_encoder_layer(x_emb).shape)

"""Encoder Block"""

class Encoder(tf.keras.layers.Layer):
  def __init__(self, *, num_layers, d_model, num_heads,
               dff, vocab_size, dropout_rate=0.1):
    super().__init__()

    self.d_model = d_model
    self.num_layers = num_layers

    self.pos_embedding = PositionalEmbedding(
        vocab_size=vocab_size, d_model=d_model)

    self.enc_layers = [
        EncoderLayer(d_model=d_model,
                     num_heads=num_heads,
                     dff=dff,
                     dropout_rate=dropout_rate)
        for _ in range(num_layers)]
    self.dropout = tf.keras.layers.Dropout(dropout_rate)

  def call(self, x):
    # `x` is token-IDs shape: (batch, seq_len)
    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.

    # Add dropout.
    x = self.dropout(x)

    for i in range(self.num_layers):
      x = self.enc_layers[i](x)

    return x  # Shape `(batch_size, seq_len, d_model)`.

# Instantiate the encoder.
sample_encoder = Encoder(num_layers=4,
                         d_model=512,
                         num_heads=8,
                         dff=2048,
                         vocab_size=VOCAB_SIZE)

sample_encoder_output = sample_encoder(x_batch_train, training=False)

# Print the shape.
print(sample_encoder_output.shape)  # Shape `(batch_size, input_seq_len, d_model)`.

"""Decoder Layer"""

class DecoderLayer(tf.keras.layers.Layer):
  def __init__(self,
               *,
               d_model,
               num_heads,
               dff,
               dropout_rate=0.1):
    super(DecoderLayer, self).__init__()

    self.causal_self_attention = CausalSelfAttention(
        num_heads=num_heads,
        key_dim=d_model,
        dropout=dropout_rate)

    self.cross_attention = CrossAttention(
        num_heads=num_heads,
        key_dim=d_model,
        dropout=dropout_rate)

    self.ffn = FeedForward(d_model, dff)

  def call(self, x, context):
    x = self.causal_self_attention(x=x)
    x = self.cross_attention(x=x, context=context)

    # Cache the last attention scores for plotting later
    self.last_attn_scores = self.cross_attention.last_attn_scores

    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.
    return x

sample_decoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=2048)

sample_decoder_layer_output = sample_decoder_layer(
    x=y_emb, context=x_emb)

print(sample_decoder_layer_output.shape)  # `(batch_size, seq_len, d_model)`

"""Decoder Block"""

class Decoder(tf.keras.layers.Layer):
  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,
               dropout_rate=0.1):
    super(Decoder, self).__init__()

    self.d_model = d_model
    self.num_layers = num_layers

    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,
                                             d_model=d_model)
    self.dropout = tf.keras.layers.Dropout(dropout_rate)
    self.dec_layers = [
        DecoderLayer(d_model=d_model, num_heads=num_heads,
                     dff=dff, dropout_rate=dropout_rate)
        for _ in range(num_layers)]

    self.last_attn_scores = None

  def call(self, x, context):
    # `x` is token-IDs shape (batch, target_seq_len)
    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)

    x = self.dropout(x)

    for i in range(self.num_layers):
      x  = self.dec_layers[i](x, context)

    self.last_attn_scores = self.dec_layers[-1].last_attn_scores

    # The shape of x is (batch_size, target_seq_len, d_model).
    return x

# Instantiate the decoder.
sample_decoder = Decoder(num_layers=4,
                         d_model=512,
                         num_heads=8,
                         dff=2048,
                         vocab_size=VOCAB_SIZE)

output = sample_decoder(
    x = y_batch_train,
    context = x_emb)

# Print the shapes.
print(x_batch_train.shape)
print(x_emb.shape)
print(output.shape)

sample_decoder.last_attn_scores.shape  # (batch, heads, target_seq, input_seq)

"""Transformer"""

class Transformer(tf.keras.Model):
  def __init__(self, *, num_layers, d_model, num_heads, dff,
               input_vocab_size, target_vocab_size, dropout_rate=0.1):
    super().__init__()
    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,
                           num_heads=num_heads, dff=dff,
                           vocab_size=input_vocab_size,
                           dropout_rate=dropout_rate)

    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,
                           num_heads=num_heads, dff=dff,
                           vocab_size=target_vocab_size,
                           dropout_rate=dropout_rate)

    self.final_layer = tf.keras.layers.Dense(target_vocab_size)

  def call(self, inputs):
    # To use a Keras model with `.fit` you must pass all your inputs in the
    # first argument.
    context, x  = inputs

    context = self.encoder(context)  # (batch_size, context_len, d_model)

    x = self.decoder(x, context)  # (batch_size, target_len, d_model)

    # Final linear layer output.
    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)

    try:
      # Drop the keras mask, so it doesn't scale the losses/metrics.
      # b/250038731
      del logits._keras_mask
    except AttributeError:
      pass

    # Return the final output and the attention weights.
    return logits

transformer = Transformer(
    num_layers = NUM_LAYERS,
    d_model = D_MODEL,
    num_heads = NUM_HEADS,
    dff = D_FF,
    input_vocab_size = VOCAB_SIZE,
    target_vocab_size = VOCAB_SIZE,
    dropout_rate = DROPOUT_RATE)

output = transformer((x_batch_train, y_batch_train))

print(y_batch_train.shape)
print(x_batch_train.shape)
print(output.shape)
print(true_y_batch_train.shape)

attn_scores = transformer.decoder.dec_layers[-1].last_attn_scores
print(attn_scores.shape)  # (batch, heads, target_seq, input_seq)

transformer.summary()

"""Learning Rate"""

class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
  def __init__(self, d_model, warmup_steps=4000):
    super().__init__()

    self.d_model = d_model
    self.d_model = tf.cast(self.d_model, tf.float32)

    self.warmup_steps = warmup_steps

  def __call__(self, step):
    step = tf.cast(step, dtype=tf.float32)
    arg1 = tf.math.rsqrt(step)
    arg2 = step * (self.warmup_steps ** -1.5)

    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)

learning_rate = CustomSchedule(D_MODEL)

optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,
                                     epsilon=1e-9)

plt.plot(learning_rate(tf.range(40000, dtype=tf.float32)))
plt.ylabel('Learning Rate')
plt.xlabel('Train Step')

"""Loss"""

def masked_loss(label, pred):
  mask = label != 0
  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True, reduction='none')
  loss = loss_object(label, pred)

  mask = tf.cast(mask, dtype=loss.dtype)
  loss *= mask

  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)
  return loss


def masked_accuracy(label, pred):
  pred = tf.argmax(pred, axis=2)
  label = tf.cast(label, pred.dtype)
  match = label == pred

  mask = label != 0

  match = match & mask

  match = tf.cast(match, dtype=tf.float32)
  mask = tf.cast(mask, dtype=tf.float32)
  return tf.reduce_sum(match)/tf.reduce_sum(mask)

"""Compile"""

transformer.compile(
    loss=masked_loss,
    optimizer=optimizer,
    metrics=[masked_accuracy])

"""Fit"""

# Fit the model using NumPy arrays
transformer.fit(train_dataset,
    epochs=200,
    validation_data=val_dataset
)

"""# **3. Testing**"""

# Define a directory to save the model
model_dir = '/content/drive/MyDrive/Saved_Models'

# Save the model
tf.saved_model.save(transformer, model_dir)

# Load a saved model (optional)
loaded_model = tf.saved_model.load(model_dir)

tokToShow = 2

x_in = np.reshape(inputX_train[0, :], (1, MAX_TOKENS))
y_in = np.reshape(inputY_train[0, 0:tokToShow], (1, tokToShow))

output = transformer((x_in, y_in), training=False)[0, -1, :].numpy()
outputTok = np.argmax(output)
outputTok

class Responder(tf.Module):
  def __init__(self, transformer):
    self.transformer = transformer

  def __call__(self, sentence, max_length=MAX_TOKENS):
    x_in = tokenize_and_pad_training([sentence])[:, 0:MAX_TOKENS]
    y_in = np.zeros((1, MAX_TOKENS)).astype(np.int32)
    startTok = Vocab[1]
    inputToks = [startTok]
    outputWords = []
    lastTok = 1
    tokToShow = 1
    while outputTok != 2 and tokToShow < MAX_TOKENS:
      y_in[0, tokToShow-1] = lastTok

sentence = 'What is the meaning of life?'
x_in = tokenize_and_pad_training([sentence])[:, 0:MAX_TOKENS]
y_in = np.zeros((1, MAX_TOKENS)).astype(np.int32)
startTok = Vocab[1]
inputToks = [startTok]
outputWords = []
lastTok = 1
tokToShow = 1
while lastTok != 2 and tokToShow < MAX_TOKENS:
  y_in[0, tokToShow-1] = lastTok
  output = transformer((x_in, y_in), training=False)[0, -1, :].numpy()
  lastTok = np.argmax(output)
  outputWords.append(Vocab[lastTok, 0])
  tokToShow += 1
print("".join(outputWords))

"""Ignore"""

sample = 0
nextTok = 0
outputToks = []
outputTok = 0

tokToShow = 1
while outputTok != 2 and tokToShow < MAX_TOKENS:
  x_in = np.reshape(inputX_train[sample, :], (1, MAX_TOKENS))
  y_in = np.reshape(inputY_train[sample, 0:tokToShow], (1, tokToShow))
  output = transformer((x_in, y_in), training=False)[0, -1, :].numpy()
  outputTok = np.argmax(output)
  outputToks.append(outputTok)
  tokToShow += 1

trueToks = np.reshape(trueY[sample, :], (1, MAX_TOKENS-1))
print(f'True Tokens: {trueToks}')
predToks = np.reshape(np.array(outputToks), (1, len(outputToks)))
print(f'Predicted Tokens: {predToks}')

"""Saving"""

from tensorflow.keras.models import load_model
transformer.save('/content/drive/MyDrive/Transformer_1.hdf5')

class Speaker(tf.Module):
  def __init__(self, transformer):
    self.tokenizers = tokenizers
    self.transformer = transformer

  def __call__(self, sentence, max_length=MAX_TOKENS):
    # The input sentence is Portuguese, hence adding the `[START]` and `[END]` tokens.
    assert isinstance(sentence, tf.Tensor)
    if len(sentence.shape) == 0:
      sentence = sentence[tf.newaxis]

    sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()

    encoder_input = sentence

    # As the output language is English, initialize the output with the
    # English `[START]` token.
    start_end = self.tokenizers.en.tokenize([''])[0]
    start = start_end[0][tf.newaxis]
    end = start_end[1][tf.newaxis]

    # `tf.TensorArray` is required here (instead of a Python list), so that the
    # dynamic-loop can be traced by `tf.function`.
    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)
    output_array = output_array.write(0, start)

    for i in tf.range(max_length):
      output = tf.transpose(output_array.stack())
      predictions = self.transformer([encoder_input, output], training=False)

      # Select the last token from the `seq_len` dimension.
      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.

      predicted_id = tf.argmax(predictions, axis=-1)

      # Concatenate the `predicted_id` to the output which is given to the
      # decoder as its input.
      output_array = output_array.write(i+1, predicted_id[0])

      if predicted_id == end:
        break

    output = tf.transpose(output_array.stack())
    # The output shape is `(1, tokens)`.
    text = tokenizers.en.detokenize(output)[0]  # Shape: `()`.

    tokens = tokenizers.en.lookup(output)[0]

    # `tf.function` prevents us from using the attention_weights that were
    # calculated on the last iteration of the loop.
    # So, recalculate them outside the loop.
    self.transformer([encoder_input, output[:,:-1]], training=False)
    attention_weights = self.transformer.decoder.last_attn_scores

    return text, tokens, attention_weights

translator = Translator(transformer)

def print_translation(sentence, tokens, ground_truth):
  print(f'{"Input:":15s}: {sentence}')
  print(f'{"Prediction":15s}: {tokens.numpy().decode("utf-8")}')
  print(f'{"Ground truth":15s}: {ground_truth}')

"""# Other Functions"""

for (batch, ((x_batch_train, y_batch_train), one_hot_y_batch_train)) in enumerate(train_dataset):
  break

def speak(input):
  global MAX_TOKENS
  responses = []
  x_in = input
  y_in = np.array([[1]])
  nextToken = np.argmax(transformer([x_in, y_in], training=False).numpy())
  nextWord = Vocab[nextToken,0]
  responses.append(nextWord)
  numResponses = 1
  while nextWord != Vocab[2,:] and numResponses < MAX_TOKENS:
    y_in = np.concatenate((y_in, np.reshape(np.array(nextToken), (1,1))), axis=1)
    nextToken = np.argmax(transformer([x_in, y_in], training=False).numpy())
    nextWord = Vocab[nextToken,0]
    responses.append(nextWord)
    numResponses += 1
  print("".join(responses))

question = np.reshape(tempX[0], (1, MAX_TOKENS))
print(speak(question))

x_in = np.reshape(tempX[0], (1, MAX_TOKENS))
y_in = np.array([[4]])
response = transformer([x_in, y_in], training=False)
response_np = response.numpy()
idx = np.argmax(response_np)
print(idx)

"""# **Extra Stuff - Reserve**"""